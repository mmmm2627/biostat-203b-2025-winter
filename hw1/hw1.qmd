---
title: "Biostat 203B Homework 1"
subtitle: Due Jan 24, 2024 @ 11:59PM
author: Sophia Luo, 106409469
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
    link-external-icon: true
    link-external-newwindow: true
---

Display machine information for reproducibility:
```{r}
#| eval: true
sessionInfo()
```

## Q1. Git/GitHub

**No handwritten homework reports are accepted for this course.** We work with Git and GitHub. Efficient and abundant use of Git, e.g., frequent and well-documented commits, is an important criterion for grading your homework.

1. Apply for the [Student Developer Pack](https://education.github.com/pack) at GitHub using your UCLA email. You'll get GitHub Pro account for free (unlimited public and private repositories).

2. Create a **private** repository `biostat-203b-2025-winter` and add `Hua-Zhou` and TA team (`Tomoki-Okuno` for Lec 1; `parsajamshidian` and `BowenZhang2001` for Lec 82) as your collaborators with write permission.

3. Top directories of the repository should be `hw1`, `hw2`, ... Maintain two branches `main` and `develop`. The `develop` branch will be your main playground, the place where you develop solution (code) to homework problems and write up report. The `main` branch will be your presentation area. Submit your homework files (Quarto file `qmd`, `html` file converted by Quarto, all code and extra data sets to reproduce results) in the `main` branch.

4. After each homework due date, course reader and instructor will check out your `main` branch for grading. Tag each of your homework submissions with tag names `hw1`, `hw2`, ... Tagging time will be used as your submission time. That means if you tag your `hw1` submission after deadline, penalty points will be deducted for late submission.

5. After this course, you can make this repository public and use it to demonstrate your skill sets on job market.

**Solution:** Done.

## Q2. Data ethics training

This exercise (and later in this course) uses the [MIMIC-IV data v3.1](https://physionet.org/content/mimiciv/3.1/), a freely accessible critical care database developed by the MIT Lab for Computational Physiology. Follow the instructions at <https://mimic.mit.edu/docs/gettingstarted/> to (1) complete the CITI `Data or Specimens Only Research` course and (2) obtain the PhysioNet credential for using the MIMIC-IV data. Display the verification links to your completion report and completion certificate here. **You must complete Q2 before working on the remaining questions.** (Hint: The CITI training takes a few hours and the PhysioNet credentialing takes a couple days; do not leave it to the last minute.)

**Solution:**

Data or Specimens Only Research 

Completion report link: <https://www.citiprogram.org/verify/?kaffd7ce3-9d0f-44c1-9b2e-a8de96beb6e0-67196202>

Certification link: 
<https://www.citiprogram.org/verify/?w29f1696a-086a-41b4-8d37-4817c26dada5-67196202>

Conflicts of Interest Certificate

Completion report link: <https://www.citiprogram.org/verify/?k272a87e7-bbe9-40db-890a-cb80fbbb40ea-67196201>

Certification link: 
<https://www.citiprogram.org/verify/?w84033b44-4d38-4426-a7fb-552b1d7d6b87-67196201>

## Q3. Linux Shell Commands

1. Make the MIMIC-IV v3.1 data available at location `~/mimic`. The output of the `ls -l ~/mimic` command should be similar to the below (from my laptop).
```{bash}
#| eval: true
# content of mimic folder
ls -l ~/mimic/
```
Refer to the documentation <https://physionet.org/content/mimiciv/3.1/> for details of data files. Do **not** put these data files into Git; they are big. Do **not** copy them into your directory. Do **not** decompress the gz data files. These create unnecessary big files and are not big-data-friendly practices. Read from the data folder `~/mimic` directly in following exercises. 

**Solution:** I downloaded the MIMIC-IV data and it's available under `~/mimic`.

  Use Bash commands to answer following questions.

2. Display the contents in the folders `hosp` and `icu` using Bash command `ls -l`. Why are these data files distributed as `.csv.gz` files instead of `.csv` (comma separated values) files? Read the page <https://mimic.mit.edu/docs/iv/> to understand what's in each folder.

**Solution:**
Content of `hosp` folder:
```{bash}
ls -l ~/mimic/hosp/
```

Content of `icu` folder:
```{bash}
ls -l ~/mimic/icu/
```

The extension `gz` means that the file is compressed. The data files have large sizes, so they have to be zipped to be downloaded faster.

3. Briefly describe what Bash commands `zcat`, `zless`, `zmore`, and `zgrep` do.

**Solution:** 

All four commands applies to files with `.gz` extension.
`zcat` displays the contents of compressed files without decompressing them to a separate file.
`zless` views the contents of compressed files in a scrollable viewer, similar to `less`.
`zmore` views the contents of compressed files in a paginated way (one screenful at a time), similar to `more`.
`zgrep` searches for a specified pattern within compressed files and outputs matching lines, similar to `grep`.

4. (Looping in Bash) What's the output of the following bash script?
```{bash}
#| eval: false
for datafile in ~/mimic/hosp/{a,l,pa}*.gz
do
  ls -l $datafile
done
```

**Solution:**
The output shows all files with names beginning with `a`, `l`, or `pa` under directory `hosp`.

Display the number of lines in each data file using a similar loop. (Hint: combine linux commands `zcat <` and `wc -l`.)

**Solution:**

Note: I use `cache` to display output from previous running to reduce the rendering time.
```{bash, cache=TRUE}
for datafile in ~/mimic/*/*.gz
do
  echo "$datafile: $(zcat < $datafile | wc -l)"
done
```

5. Display the first few lines of `admissions.csv.gz`. How many rows are in this data file, excluding the header line? Each `hadm_id` identifies a hospitalization. How many hospitalizations are in this data file? How many unique patients (identified by `subject_id`) are in this data file? Do they match the number of patients listed in the `patients.csv.gz` file? (Hint: combine Linux commands `zcat <`, `head`/`tail`, `awk`, `sort`, `uniq`, `wc`, and so on.)

**Solution:**

The first few lines of `admissions.csv.gz`:
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | head
```

The number of rows in this data file, excluding the header line:
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | tail -n +2 | wc -l
```

Note: 
- `uniq` detects duplicates only if they are next to each other, so we need to `sort` first.
- `tail -n +2` is used to exclude the header.

The number of hospitalizations in this data file:
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz |
tail -n +2 |
cut -d, -f2 | 
sort |
uniq |
wc -l
```
Alternatively using `awk`:
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz |
tail -n +2 |
awk -F, '{print $2}' | 
sort |
uniq |
wc -l
```

Peek the first few lines of `patients.csv.gz`:
```{bash}
zcat < ~/mimic/hosp/patients.csv.gz | head
```

The number of unique patients in `admissions.csv.gz` is
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz |
tail -n +2 |
awk -F, '{print $1}' |
sort |
uniq |
wc -l
```
which is less than the number of patients listed in the `patients.csv.gz` file:
```{bash}
zcat < ~/mimic/hosp/patients.csv.gz |
tail -n +2 |
awk -F, '{print $1}' |
sort |
uniq |
wc -l
```

6. What are the possible values taken by each of the variable `admission_type`, `admission_location`, `insurance`, and `ethnicity`? Also report the count for each unique value of these variables in decreasing order. (Hint: combine Linux commands `zcat`, `head`/`tail`, `awk`, `uniq -c`, `wc`, `sort`, and so on; skip the header line.)

**Solution:**

The possible values taken by `admission_type` and the count in decreasing order:
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz |
tail -n +2 |
awk -F, '{print $6}' |
sort |
uniq -c |
sort -r
```

The possible values taken by `admission_location` and the count in decreasing order:
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz |
tail -n +2 |
awk -F, '{print $8}' |
sort |
uniq -c |
sort -r
```

The possible values taken by `insurance` and the count in decreasing order:
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz |
tail -n +2 |
awk -F, '{print $10}' |
sort |
uniq -c |
sort -r
```

The possible values taken by `ethnicity` (`race` in the file) and the count in decreasing order:
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz |
tail -n +2 |
awk -F, '{print $13}' |
sort |
uniq -c |
sort -r
```

7. The `icusays.csv.gz` file contains all the ICU stays during the study period. How many ICU stays, identified by `stay_id`, are in this data file? How many unique patients, identified by `subject_id`, are in this data file?

**Solution:**

Peek the first few lines of the file:
```{bash}
zcat < ~/mimic/icu/icustays.csv.gz | head
```

The number of ICU stays identified by `stay_id`:
```{bash}
zcat < ~/mimic/icu/icustays.csv.gz |
tail -n +2 |
awk -F, '{print $3}' |
sort |
uniq |
wc -l
```

The number of unique patients identified by `subject_id`:
```{bash}
zcat < ~/mimic/icu/icustays.csv.gz |
tail -n +2 |
awk -F, '{print $1}' |
sort |
uniq |
wc -l
```

8. _To compress, or not to compress. That's the question._ Let's focus on the big data file `labevents.csv.gz`. Compare compressed gz file size to the uncompressed file size. Compare the run times of `zcat < ~/mimic/labevents.csv.gz | wc -l` versus `wc -l labevents.csv`. Discuss the trade off between storage and speed for big data files. (Hint: `gzip -dk < FILENAME.gz > ./FILENAME`. Remember to delete the large `labevents.csv` file after the exercise.)

**Solution:**

I runned `gzip -dk labevents.csv.gz ./labevents.csv` to decompress the file into `labevents.csv`.

File size comparison:
```{bash, cache=TRUE}
ls -lh ~/mimic/hosp/labevents*
```

The uncompressed file (18G) is more than 7 times larger than the compressed file (2.5G).

Note: I use `cache` to display output from previous running to reduce the rendering time.

The run time of `zcat | wc` on compressed file:
```{bash, cache=TRUE}
time zcat < ~/mimic/hosp/labevents.csv.gz | wc -l
```

The run time of `wc` on uncompressed file:
```{bash, cache=TRUE}
time wc -l ~/mimic/hosp/labevents.csv
```

The runtime of compressed file is 1 min 38 secs, compared to uncompressed file of 4m 28 secs.

The expected tradeoff is to balance storage space and run time.
Compressed files are much smaller than the uncompressed counterparts, significantly reducing storage requirement. However, operating on compressed files is slower because the file must be decompressed first.
The uncompressed files operation is faster since the file is directly accessible without decompression, but it takes lots of storage space.

My result deviates from the expectation, and here's my thoughts of the potential reasons:

a. Disk I/O Bottleneck: The file needs to be read from disk. For large uncompressed files, the time it takes to read the file from disk dominates the runtime. Even though there's no decompression step, the sheer volume of data in the uncompressed file makes the operation slower.

b. Efficient Decompression: Gzip algorithms are highly optimized for decompression, so the CPU overhead for decompression might be negligible compared to the disk I/O savings.

The large uncompressed file is deleted using the command `rm ~/mimic/hosp/labevents.csv`.

## Q4. Who's popular in Price and Prejudice

1. You and your friend just have finished reading *Pride and Prejudice* by Jane Austen. Among the four main characters in the book, Elizabeth, Jane, Lydia, and Darcy, your friend thinks that Darcy was the most mentioned. You, however, are certain it was Elizabeth. Obtain the full text of the novel from <http://www.gutenberg.org/cache/epub/42671/pg42671.txt> and save to your local folder. 
```{bash}
#| eval: false
wget -nc http://www.gutenberg.org/cache/epub/42671/pg42671.txt
```
Explain what `wget -nc` does. Do **not** put this text file `pg42671.txt` in Git. Complete the following loop to tabulate the number of times each of the four characters is mentioned using Linux commands.

**Solution:**

`wget` downloads the files from the web using the link provided by users. `-nc` specifies that `wget` will not overwrite an existing file of the same name in the current directory. Therefore, when the file is already downloaded, the content won't be retrieved again.

```{bash}
#| eval: false
wget -nc http://www.gutenberg.org/cache/epub/42671/pg42671.txt
for char in Elizabeth Jane Lydia Darcy
do
  count=$(grep -o "$char" "pg42671.txt" | wc -l)
  echo "$char: $count" 
done
```

2. What's the difference between the following two commands?
```{bash}
#| eval: false
echo 'hello, world' > test1.txt
```
and
```{bash}
#| eval: false
echo 'hello, world' >> test2.txt
```

**Solution:**

Both `>` and `>>` are redirection operators. They both create the file if it doesn't exist.
The main difference is that when running the command again while content exist, `>` will overwrite existing content but `>>` will preserve existing content and adds new content at the end.
For example, after running the above command for 4 times, `test1.txt` will only have "hello, world" in first line, but `test2.txt` will have 4 "hello, world" in 4 lines.

3. Using your favorite text editor (e.g., `vi`), type the following and save the file as `middle.sh`:
```{bash eval=FALSE}
#!/bin/sh
# Select lines from the middle of a file.
# Usage: bash middle.sh filename end_line num_lines
head -n "$2" "$1" | tail -n "$3"
```
Using `chmod` to make the file executable by the owner, and run
```{bash}
#| eval: false
./middle.sh pg42671.txt 20 5
```
Explain the output. Explain the meaning of `"$1"`, `"$2"`, and `"$3"` in this shell script. Why do we need the first line of the shell script?

**Solution:**

When executing `./middle.sh pg42671.txt 20 5`, the `.sh` file is executed with three parameters passed in:
$1: pg42671.txt
$2: 20
$3: 5
Then in `middle.sh` file, when `head` and `tail` command are run, they look for the actual `$1`, `$2`, `$3` sent along with the file. This is similar to running a function, where we specifies the parameters when calling the function, and within the function, we use temporary variable names instead of the actual variables.

The first line of the shell script `#!/bin/sh` is shebang or hashbang, and it specifies which shell or interpreter should execute the script. Here, it tells the system to use the Bourne shell `sh` to execute the script.

## Q5. More fun with Linux

Try following commands in Bash and interpret the results: `cal`, `cal 2025`, `cal 9 1752` (anything unusual?), `date`, `hostname`, `arch`, `uname -a`, `uptime`, `who am i`, `who`, `w`, `id`, `last | head`, `echo {con,pre}{sent,fer}{s,ed}`, `time sleep 5`, `history | tail`.

**Solution:**

```{bash}
cal
```
`cal`: prints out calender of current month.

```{bash}
cal 2025
```
`cal 2025`: prints out calender of 2025.

```{bash}
cal 9 1752
```
`cal 9 1752`: prints out calender of September of 1752 but it's missing 3rd to 13th.

```{bash}
date
```
`date`: prints out current date, time, and time zone.

```{bash}
hostname
```
`hostname`: prints out name of the host.

```{bash}
arch
```
`arch` shows machine architecture of the system, including hardware platform and processor architecture.
Here it shows that my laptop is using 64-bit architecture.

```{bash}
uname -a
```
`uname` displays all available information about my system.

```{bash}
uptime
```
`uptime` shows how long the system has been running, along with information about the system's load averages. Here it shows that the system has been up for over 3 hours and 30 minutes.

```{bash}
whoami
```
`whoami` shows the current logged-in user's username.

```{bash}
who
```
`who` displays information about the currently logged-in user, the terminal, and the time at which the user logged in.

```{bash}
w
```
`w` provides detailed information about the users currently logged in, activities, and system information such as uptime and load averages.

```{bash}
id
```
`id` displays the user and group information for the current user or a specified user.

```{bash}
last | head
```
`last` shows a list of the most recent logins on the system.


```{bash}
echo {con,pre}{sent,fer}{s,ed}
```
This prints out all combinations of contents in the three brackets in order.

```{bash}
time sleep 5
```
`sleep` pauses the execution of a script or command for a specified duration, in this case, 5 secs.
`time` shows that `sleep 5` took about 6 secs to run, so the execution did pause for 5 secs.

`history` display the most recent commands that have been executed in the current terminal session..

## Q6. Book

1. Git clone the repository <https://github.com/christophergandrud/Rep-Res-Book> for the book _Reproducible Research with R and RStudio_ to your local machine. Do **not** put this repository within your homework repository `biostat-203b-2025-winter`.

2. Open the project by clicking `rep-res-3rd-edition.Rproj` and compile the book by clicking `Build Book` in the `Build` panel of RStudio. (Hint: I was able to build `git_book` and `epub_book` directly. For `pdf_book`, I needed to add a line `\usepackage{hyperref}` to the file `Rep-Res-Book/rep-res-3rd-edition/latex/preabmle.tex`.)

The point of this exercise is (1) to obtain the book for free and (2) to see an example how a complicated project such as a book can be organized in a reproducible way. Use `sudo apt install PKGNAME` to install required Ubuntu packages and `tlmgr install PKGNAME` to install missing TexLive packages.

For grading purpose, include a screenshot of Section 4.1.5 of the book here.